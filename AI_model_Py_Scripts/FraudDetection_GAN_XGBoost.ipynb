{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ddba63",
   "metadata": {},
   "source": [
    "# Fraud Detection with GAN Augmentation + XGBoost\n",
    "\n",
    "This notebook contains GAN + downstream classifier pipeline with XGBoost.\n",
    "\n",
    "Steps:\n",
    "- Import libraries and load dataset\n",
    "- Identify feature types and preprocess (scale + one-hot encode)\n",
    "- Split into train/test\n",
    "- Build and train a simple GAN to generate synthetic samples\n",
    "- Generate synthetic data and visualize diagnostics\n",
    "- Augment the training set and compute class imbalance weight\n",
    "- Train/validate an XGBoost classifier (with optional tuning)\n",
    "- Evaluate metrics and plots\n",
    "- Save model as `xg_model.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Environment Setup\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Try to ensure xgboost is available (optional)\n",
    "try:\n",
    "    import xgboost as xgb  # noqa: F401\n",
    "except Exception:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    except Exception as e:\n",
    "        print(\"Warning: Failed to install xgboost, proceeding if already available:\", e)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset robustly\n",
    "from pathlib import Path\n",
    "\n",
    "candidate_paths = [\n",
    "    Path('fraud_dataset_Generator_using_numpy.csv'),\n",
    "    Path('../AI_model_Py_Scripts/fraud_dataset_Generator_using_numpy.csv'),\n",
    "    Path('./fraud_dataset.csv'),\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for p in candidate_paths:\n",
    "    if p.exists():\n",
    "        csv_path = p\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\"Could not find dataset in candidate paths: \" + \", \".join(map(str, candidate_paths)))\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "print(\"Loaded:\", csv_path)\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "if 'Label' in data.columns:\n",
    "    print(data['Label'].value_counts())\n",
    "else:\n",
    "    raise KeyError(\"Expected 'Label' column in dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: scale numerical, one-hot encode categorical\n",
    "\n",
    "numerical_cols = data.select_dtypes(include=[\"float64\", \"int64\"]).columns.drop('Label', errors='ignore')\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Preprocessed head:\\n\", data.head())\n",
    "\n",
    "# Split X/y\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "print(\"Class distribution (train):\\n\", y_train.value_counts())\n",
    "print(\"Class distribution (test):\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simple GAN (generator, discriminator, gan)\n",
    "\n",
    "latent_dim = 100\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Generator\n",
    "def build_generator(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compose GAN\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = tf.keras.models.Model(gan_input, gan_output)\n",
    "    return gan\n",
    "\n",
    "generator = build_generator(latent_dim, input_dim)\n",
    "discriminator = build_discriminator(input_dim)\n",
    "\n",
    "discriminator.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "print(\"GAN built. Input dim:\", input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAN\n",
    "import numpy as np\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "half_batch = batch_size // 2\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, gan, X_train, epochs, batch_size, latent_dim):\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_data = X_train.iloc[idx].values.astype(np.float32)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        synthetic_data = generator.predict(noise, verbose=0)\n",
    "\n",
    "        real_labels = np.random.uniform(0.9, 1.0, (half_batch, 1)).astype(np.float32)\n",
    "        fake_labels = np.random.uniform(0.0, 0.1, (half_batch, 1)).astype(np.float32)\n",
    "\n",
    "        real_data += np.random.normal(0, 0.01, real_data.shape)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_data, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(synthetic_data, fake_labels)\n",
    "\n",
    "        # Train Generator (via GAN)\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            sample_noise = np.random.normal(0, 1, (5, latent_dim))\n",
    "            gen_samples = generator.predict(sample_noise, verbose=0)\n",
    "            print(f\"Epoch {epoch}: D_real={d_loss_real[0]:.4f}, D_fake={d_loss_fake[0]:.4f}, G={g_loss:.4f}, range=({gen_samples.min():.3f},{gen_samples.max():.3f})\")\n",
    "\n",
    "train_gan(generator, discriminator, gan, X_train, epochs, batch_size, latent_dim)\n",
    "print(\"GAN training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples equal to |X_train|\n",
    "num_samples = X_train.shape[0]\n",
    "noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
    "synthetic_data = generator.predict(noise, verbose=0).astype(np.float32)\n",
    "print(\"Synthetic generated:\", synthetic_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual diagnostics: histograms and KDE for a few features\n",
    "plt.figure(figsize=(12, 5))\n",
    "num_features = min(2, X_train.shape[1])\n",
    "real_subset = X_train.iloc[:, :num_features].values\n",
    "synth_subset = synthetic_data[:, :num_features]\n",
    "\n",
    "for i in range(num_features):\n",
    "    plt.subplot(1, num_features, i + 1)\n",
    "    plt.hist(real_subset[:, i], bins=40, alpha=0.5, label='Real', color='blue')\n",
    "    plt.hist(synth_subset[:, i], bins=40, alpha=0.5, label='Synthetic', color='red')\n",
    "    sns.kdeplot(real_subset[:, i], color='blue', label='Real KDE', fill=True, alpha=0.2)\n",
    "    sns.kdeplot(synth_subset[:, i], color='red', label='Synthetic KDE', fill=True, alpha=0.2)\n",
    "    plt.title(f\"Feature {i+1} Distribution\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison: means and variances\n",
    "real_mean = np.mean(X_train, axis=0)\n",
    "real_var = np.var(X_train, axis=0)\n",
    "synth_mean = np.mean(synthetic_data, axis=0)\n",
    "synth_var = np.var(synthetic_data, axis=0)\n",
    "print(\"Mean diff (|real - synth|) sample:\", np.abs(real_mean - synth_mean)[:5])\n",
    "print(\"Var  diff (|real - synth|) sample:\", np.abs(real_var - synth_var)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build augmented dataset and labels\n",
    "num_synth = synthetic_data.shape[0]\n",
    "fraud_ratio = float(np.mean(y_train))\n",
    "num_fraud_samples = int(fraud_ratio * num_synth)\n",
    "\n",
    "synthetic_labels = np.zeros((num_synth, 1), dtype=np.int32)\n",
    "synthetic_labels[:num_fraud_samples] = 1\n",
    "\n",
    "# y_train to 2D\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "\n",
    "X_train_augmented = np.concatenate((X_train.values, synthetic_data), axis=0)\n",
    "y_train_augmented = np.concatenate((y_train_2d, synthetic_labels), axis=0)\n",
    "\n",
    "print(\"Augmented shapes:\", X_train_augmented.shape, y_train_augmented.shape)\n",
    "\n",
    "# Compute scale_pos_weight = N_neg / N_pos\n",
    "pos = int(y_train_augmented.sum())\n",
    "neg = int(y_train_augmented.shape[0] - pos)\n",
    "scale_pos_weight = (neg / max(1, pos))\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation split for augmented data\n",
    "y_aug_1d = y_train_augmented.ravel()\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_augmented, y_aug_1d, test_size=0.2, random_state=42, stratify=y_aug_1d\n",
    ")\n",
    "print(\"Train/Val shapes:\", X_train_split.shape, X_val_split.shape, y_train_split.shape, y_val_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e432978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/Import XGBoost (idempotent)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception as e:\n",
    "    print(\"If xgboost import fails, please install it via pip.\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGBoost training\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train_split, y_train_split)\n",
    "\n",
    "y_pred_val = xgb_clf.predict(X_val_split)\n",
    "acc = accuracy_score(y_val_split, y_pred_val)\n",
    "print(f\"Baseline XGBoost Accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV (optional - can be time-consuming)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 1],\n",
    "    'reg_alpha': [0.0, 0.1],\n",
    "    'reg_lambda': [1.0, 2.0],\n",
    "}\n",
    "\n",
    "base = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "gs.fit(X_train_split, y_train_split)\n",
    "print(\"Best params:\", gs.best_params_)\n",
    "print(\"Best CV ROC-AUC:\", gs.best_score_)\n",
    "\n",
    "best_xgb = gs.best_estimator_\n",
    "best_xgb.fit(X_train_split, y_train_split)\n",
    "print(\"Refit best XGB on training split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec65535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics and plots (using best_xgb if available, else xgb_clf)\n",
    "model = globals().get('best_xgb', None) or xgb_clf\n",
    "\n",
    "# Predictions and probabilities\n",
    "y_pred = model.predict(X_val_split)\n",
    "y_proba = model.predict_proba(X_val_split)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_val_split, y_pred)\n",
    "prec = precision_score(y_val_split, y_pred)\n",
    "rec = recall_score(y_val_split, y_pred)\n",
    "f1 = f1_score(y_val_split, y_pred)\n",
    "auc = roc_auc_score(y_val_split, y_proba)\n",
    "\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall: {rec:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc:.3f}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val_split, y_proba)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr, tpr, label=f'ROC (AUC={auc:.3f})', color='blue')\n",
    "plt.plot([0,1], [0,1], 'k--', alpha=0.6)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - XGBoost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val_split, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud','Fraud'], yticklabels=['Not Fraud','Fraud'])\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ac2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost model as xg_model.pkl\n",
    "import pickle\n",
    "\n",
    "final_model = globals().get('best_xgb', None) or xgb_clf\n",
    "with open('xg_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(\"Model saved as 'xg_model.pkl'\")\n",
    "\n",
    "# Optional: auto-download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('xg_model.pkl')\n",
    "except Exception:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
